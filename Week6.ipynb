{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "448fe916-3ca2-4f7c-a790-3ba77af01140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test size: 0.2, Mean Squared Error: 2900.1936\n",
      "Test size: 0.3, Mean Squared Error: 2821.7510\n",
      "Test size: 0.4, Mean Squared Error: 2832.9962\n",
      "\n",
      "Comparison of Model Performance:\n",
      "Test size: 0.2 -> MSE: 2900.1936\n",
      "Test size: 0.3 -> MSE: 2821.7510\n",
      "Test size: 0.4 -> MSE: 2832.9962\n"
     ]
    }
   ],
   "source": [
    "#Week 5 Programs\n",
    "\n",
    "'''27. Experiment with different test_size values (e.g., 0.2, 0.3, 0.4) and observe model performance on diabetes dataset'''\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Load Diabetes dataset\n",
    "diabetes = load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "# Function to train the model and evaluate performance\n",
    "def evaluate_model(test_size):\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    # Initialize the model\n",
    "    model = LinearRegression()\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    # Calculate the Mean Squared Error (MSE) for model performance\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    return mse\n",
    "# Experiment with different test_size values: 0.2, 0.3, 0.4\n",
    "test_sizes = [0.2, 0.3, 0.4]\n",
    "results = {}\n",
    "for size in test_sizes:\n",
    "    mse = evaluate_model(size)\n",
    "    results[size] = mse\n",
    "    print(f\"Test size: {size}, Mean Squared Error: {mse:.4f}\")\n",
    "# Comparison of model performance\n",
    "print(\"\\nComparison of Model Performance:\")\n",
    "for size, mse in results.items():\n",
    "    print(f\"Test size: {size} -> MSE: {mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4dc69fb-9ed4-4f2a-b2ea-83c89966589c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random State: 0.2 -> MSE: 2900.1936\n",
      "Random State: 0.3 -> MSE: 2821.7510\n",
      "Random State: 0.4 -> MSE: 2832.9962\n"
     ]
    }
   ],
   "source": [
    "'''28. Use a fixed random_state value for reproducibility when splitting data, and test with different random_state values. '''\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Load Diabetes dataset\n",
    "diabetes = load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "# Function to train the model and evaluate performance\n",
    "def evaluate_model(random_state_value):\n",
    "    # Split the dataset into training and testing sets with a specific random_state\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state_value)\n",
    "    # Initialize the model\n",
    "    model = LinearRegression()\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    # Calculate the Mean Squared Error (MSE) for model performance\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    return mse\n",
    "    # Experiment with different random_state values: 42, 0, 100, None\n",
    "    random_state_values = [42, 0, 100, None]\n",
    "    results = {}\n",
    "    for state in random_state_values:\n",
    "        mse = evaluate_model(state)\n",
    "        results[state] = mse\n",
    "        print(f\"Random State: {state}, Mean Squared Error: {mse:.4f}\")\n",
    "        # Comparison of model performance for different random states\n",
    "        print(\"\\nComparison of Model Performance:\")\n",
    "for state, mse in results.items():\n",
    "    print(f\"Random State: {state} -> MSE: {mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b90d2d5f-58f0-4ade-bcb1-7be64f2853eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each feature:\n",
      "age    0\n",
      "sex    0\n",
      "bmi    0\n",
      "bp     0\n",
      "s1     0\n",
      "s2     0\n",
      "s3     0\n",
      "s4     0\n",
      "s5     0\n",
      "s6     0\n",
      "dtype: int64\n",
      "\n",
      "Preprocessed Training Features (first 5 rows):\n",
      "          age       sex       bmi        bp        s1            s2        s3  \\\n",
      "17   1.513010  1.098506  0.257665  1.213660  0.774155  1.148366e-17 -0.843411   \n",
      "66  -0.193150  1.098506 -0.393807 -0.701413 -0.419833  2.595900e-01 -1.555385   \n",
      "137  0.117061 -0.964904  1.072006  2.097540 -0.300434 -3.517100e-01 -0.131437   \n",
      "245 -0.580914 -0.964904 -0.766077 -0.627756 -1.195925 -1.258584e+00  0.659645   \n",
      "31  -0.503361 -0.964904 -1.417550 -1.732606 -0.807879 -1.151102e+00  1.292511   \n",
      "\n",
      "           s4            s5            s6  \n",
      "17   0.771544  5.840213e-01 -2.362735e-17  \n",
      "66   1.577289  1.002237e-03  4.000295e-01  \n",
      "137 -0.034202  3.617600e-01 -3.221016e-01  \n",
      "245 -0.839948 -1.078132e+00 -2.849561e+00  \n",
      "31  -1.645693  1.924921e-17 -9.539664e-01  \n",
      "\n",
      "Preprocessed Test Features (first 5 rows):\n",
      "          age       sex       bmi        bp        s1        s2        s3  \\\n",
      "287  0.970141 -0.964904 -0.137872 -0.333129  2.744235  2.684638  0.422321   \n",
      "211  1.978326 -0.964904  0.792804  0.477094 -0.509382 -0.358428  0.026779   \n",
      "72   1.357905  1.098506 -0.091338 -0.259473  2.266640  1.045547  1.213403   \n",
      "321  2.055879 -0.964904  1.118540  1.704950  1.221900  0.783561 -1.634493   \n",
      "73   0.272166  1.098506 -0.440341 -0.038503  0.863704  1.139593 -0.131437   \n",
      "\n",
      "           s4        s5        s6  \n",
      "287  0.771544  0.693092 -0.141569  \n",
      "211 -0.839948 -0.489424 -0.502634  \n",
      "72  -0.034202  1.813423 -0.412368  \n",
      "321  3.108206  2.118076  1.302693  \n",
      "73   0.771544 -0.115523 -0.231835  \n"
     ]
    }
   ],
   "source": [
    "'''29. Clean and preprocess a dataset (handle missing values, scale\n",
    "features) before splitting into training and test sets'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "# Load the Diabetes dataset\n",
    "diabetes = load_diabetes()\n",
    "X = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
    "y = pd.Series(diabetes.target)\n",
    "# Step 1: Check for missing values\n",
    "print(\"Missing values in each feature:\")\n",
    "print(X.isnull().sum())\n",
    "# Step 2: Handle missing values by imputation (if any)\n",
    "# For demonstration, let's assume some missing values are introduced randomly\n",
    "# Introduce missing values randomly in 5% of the data for each feature\n",
    "np.random.seed(42)\n",
    "missing_rate = 0.05\n",
    "n_missing = int(missing_rate * X.size)\n",
    "missing_indices = np.random.choice(X.size, n_missing, replace=False)\n",
    "X.values.ravel()[missing_indices] = np.nan\n",
    "# Impute missing values using the mean strategy\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "# Step 3: Scale the features using Standardization\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X_imputed), columns=X.columns)\n",
    "# Step 4: Split into training and test sets (80% training, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "# Show the preprocessed data\n",
    "print(\"\\nPreprocessed Training Features (first 5 rows):\")\n",
    "print(X_train.head())\n",
    "print(\"\\nPreprocessed Test Features (first 5 rows):\")\n",
    "print(X_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4151499f-8f09-4318-9260-9642e3254c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with scaling before splitting: 0.5169\n",
      "Accuracy with scaling after splitting: 0.7303\n"
     ]
    }
   ],
   "source": [
    "'''30. Compare results when applying feature scaling before or after splitting the data. '''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Load the diabetes dataset\n",
    "data = load_diabetes()\n",
    "X = data.data\n",
    "y = (data.target > np.median(data.target)).astype(int) # Convert target to binary as 0 or 1\n",
    "# Function to train and evaluate model with scaling before splitting\n",
    "def model_with_scaling_before_split(X, y):\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    # Apply feature scaling to the entire dataset before splitting\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X) # Scale the whole dataset\n",
    "    # Split the scaled data\n",
    "    X_train_scaled, X_test_scaled = X_scaled[:len(X_train)], X_scaled[len(X_train):]\n",
    "    # Train the model\n",
    "    model = LogisticRegression(max_iter=200)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    # Predict and evaluate the model\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy\n",
    "# Function to train and evaluate model with scaling after splitting\n",
    "def model_with_scaling_after_split(X, y):\n",
    "    # Split the data into training and testing sets (80% train, 20% test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    # Apply feature scaling after splitting\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train) # Scale only the training set\n",
    "    X_test_scaled = scaler.transform(X_test) # Transform the test set based on training data\n",
    "    # Train the model\n",
    "    model = LogisticRegression(max_iter=200)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    # Predict and evaluate the model\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy\n",
    "    # Compare the results\n",
    "accuracy_before_split = model_with_scaling_before_split(X, y)\n",
    "accuracy_after_split = model_with_scaling_after_split(X, y)\n",
    "print(f\"Accuracy with scaling before splitting: {accuracy_before_split:.4f}\")\n",
    "print(f\"Accuracy with scaling after splitting: {accuracy_after_split:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "764a6e6f-862c-4766-88af-577fbc4902e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Write a Python program using Scikit-learn to split the iris dataset into 70% train data and 30% test data. Out of total 150 records, the training set will contain 120 records and the test set contains 30 of those records. Print both datasets.\\n(Students only must write the program for this Assessment Question and should execute in Lab)'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Assessment 3:\n",
    "'''Write a Python program using Scikit-learn to split the iris dataset into 70% train data and 30% test data. Out of total 150 records, the training set will contain 120 records and the test set contains 30 of those records. Print both datasets.\n",
    "(Students only must write the program for this Assessment Question and should execute in Lab)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea82b818-47b6-44d3-8752-445e383ffcbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
